{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Katakana classifier\n",
    "\n",
    "- This notebook will demonstrate how to build a CNN model to recognize Japanese katakana characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "- Python packages: `numpy`, `opencv-python`, `matplotlib`, `pillow`, `pandas` and `tensorflow`\n",
    "\n",
    "- Install all of them with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install numpy opencv-python matplotlib pillow pandas tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- then import all necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import struct\n",
    "\n",
    "# set GPU to invisible for saving battery life\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print('TensorFlow version:',tf.VERSION)\n",
    "print('Keras version:', keras.__version__)\n",
    "\n",
    "# tf.enable_eager_execution() # unable to use model.save() while enabling eager execution\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the datasets\n",
    "\n",
    "- Require the [ETL-1](http://etlcdb.db.aist.go.jp/) to be un-zipped\n",
    "\n",
    "```bash\n",
    "├───ETL1\n",
    "│───────ETL1C_01\n",
    "│───────ETL1C_02\n",
    "│───────ETL1C_03\n",
    "│───────ETL1C_04\n",
    "│───────ETL1C_05\n",
    "│───────ETL1C_06\n",
    "│───────ETL1C_07\n",
    "│───────ETL1C_08\n",
    "│───────ETL1C_09\n",
    "│───────ETL1C_10\n",
    "│───────ETL1C_11\n",
    "│───────ETL1C_12\n",
    "│───────ETL1C_13\n",
    "│───────ETL1INFO\n",
    "```\n",
    "\n",
    "- The [`label_dict.csv`](label_dict.csv) file contains the `index`, `label`, and `kana` representation information.\n",
    "\n",
    "- We are going to perform the following steps:\n",
    "\n",
    "    1. Convert ETL datasets to categozied images. (*once time*)\n",
    "    2. Convert the categozied images to `numpy` arrays then save them as `.npy` files. (*once time*)\n",
    "    3. Load them into memory to perform training. (*every time when re-run the notebook*)\n",
    "        - This step requires lots of memory (~2GB) so if you can convert them to `tf.data.Dataset` before training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert ETL datasets to categorized images\n",
    "\n",
    "- We will unpack the ETL datasets into images and save them in separated categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_etl_file(etl_file, unpack_dir):\n",
    "    idx = 0\n",
    "    print('Unpacking {}'.format(etl_file))\n",
    "    \n",
    "    f = open(etl_file, 'rb')\n",
    "    while True:\n",
    "        \n",
    "        idx += 1\n",
    "        if idx % 100 == 0:\n",
    "            print('.', end='')\n",
    "        \n",
    "        s = f.read(2052)\n",
    "\n",
    "        if not len(s) == 2052:\n",
    "            print()\n",
    "            print('[{}] Reach EOF, remain {} bytes unread.'.format(etl_file, len(s)))\n",
    "            break;\n",
    "\n",
    "        r = struct.unpack('>H2sH6BI4H4B4x2016s4x', s)\n",
    "\n",
    "        # the record index is at index 0. We are going to use it as file name\n",
    "        r_idx = r[0]\n",
    "\n",
    "        # label at index 1\n",
    "        label = r[1].decode('ascii')\n",
    "        if ' ' in label: # remove spaces\n",
    "            label = label.replace(' ', '')\n",
    "\n",
    "        # image at index 18\n",
    "        iF = Image.frombytes('F', (64, 63), r[18], 'bit', 4)\n",
    "        np_img = np.array(iF, dtype=np.uint8) # np_img.shape = (63, 64)\n",
    "        final_img = np_img.astype(np.float) * int(255 / 15)\n",
    "        \n",
    "        cat_dir = os.path.join(unpack_dir, label)\n",
    "        \n",
    "        if not os.path.exists(cat_dir):\n",
    "            os.makedirs(cat_dir)\n",
    "            \n",
    "        img_path = os.path.join(cat_dir, '{}.png'.format(r_idx))\n",
    "        \n",
    "        if not os.path.exists(img_path):\n",
    "            cv2.imwrite(img_path, final_img)\n",
    "\n",
    "    f.close()\n",
    "\n",
    "def etl_to_images(etl_root='ETL1', unpack_dir='etlcb_images'):\n",
    "    # we only need the katakana characters\n",
    "    skip_etls = [\n",
    "        'ETL1C_01',\n",
    "        'ETL1C_02',\n",
    "        'ETL1C_03',\n",
    "        'ETL1C_04',\n",
    "        'ETL1C_05',\n",
    "        'ETL1C_06',\n",
    "    ]\n",
    "    \n",
    "    etl_files = os.listdir(etl_root)\n",
    "    \n",
    "    for etl in etl_files:\n",
    "        if not re.search('ETL1C_\\d\\d', etl) == None:\n",
    "            if etl in skip_etls:\n",
    "                continue\n",
    "                \n",
    "            etl_file = os.path.join(etl_root, etl)\n",
    "            \n",
    "            unpack_etl_file(etl_file, unpack_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etl_to_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the ETL images into `.npy` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_etl_images_to_numpy(etl_image_root='etlcb_images', etl_numpy_root='etlcb_numpy', input_shape=(64, 64, 1), csv_dict_path='label_dict.csv', img_filename='img_ds.npy', label_filename='label_ds.npy'):\n",
    "    label_dict_df = pd.read_csv(csv_dict_path, keep_default_na=False)\n",
    "    label_dict_df = label_dict_df.set_index('label')\n",
    "    \n",
    "    label_dict = label_dict_df.to_dict(orient='index')\n",
    "    kana_dirs = os.listdir(etl_image_root)\n",
    "    \n",
    "    for kana_dir in kana_dirs:\n",
    "        \n",
    "        img_arr = []\n",
    "        label_arr = []\n",
    "        \n",
    "        try:\n",
    "            dir_label = int(label_dict[kana_dir]['index'])\n",
    "        except KeyError:\n",
    "            continue\n",
    "        \n",
    "        img_paths = [os.path.join(etl_image_root, kana_dir, img) for img in os.listdir(os.path.join(etl_image_root, kana_dir))]\n",
    "#         print(img_paths)\n",
    "        for img_path in img_paths:\n",
    "            img = cv2.imread(img_path, 0) # read as grayscale image\n",
    "            img = cv2.resize(img, input_shape[:2], interpolation=cv2.INTER_CUBIC)\n",
    "            img = img.reshape(input_shape)\n",
    "            img = img.astype(np.float) / 255.0\n",
    "#             print(img.shape)\n",
    "            img_arr.append(img)\n",
    "            label_arr.append(dir_label)\n",
    "        \n",
    "        kana_np_path = os.path.join(etl_numpy_root, kana_dir)\n",
    "        if not os.path.exists(kana_np_path):\n",
    "            os.makedirs(kana_np_path)\n",
    "            \n",
    "        img_arr = np.array(img_arr)\n",
    "        print('img_arr.shape', img_arr.shape)\n",
    "        label_arr = np.array(label_arr)\n",
    "        print('label_arr.shape', label_arr.shape)\n",
    "        \n",
    "        img_ds_path = os.path.join(kana_np_path, 'img_ds.npy')\n",
    "        np.save(img_ds_path, img_arr)\n",
    "        label_ds_path = os.path.join(kana_np_path, 'label_ds.npy')\n",
    "        np.save(label_ds_path, label_arr)\n",
    "        \n",
    "        del img_arr\n",
    "        del label_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_etl_images_to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data for training\n",
    "\n",
    "- The `load_numpy_etl()` method will return a `dict` which contains the following objects:\n",
    "    - `train_set`: the training data\n",
    "    - `train_label`: the training labels\n",
    "    - `val_set`: the validation data\n",
    "    - `val_label`: the validation labels\n",
    "    - `label_dict`: a `dict` contains the list of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_numpy_etl(etl_numpy_root='etlcb_numpy', csv_dict='label_dict.csv', val_ratio=0.7):\n",
    "    retval = dict()\n",
    "    \n",
    "    label_dict_df = pd.read_csv(csv_dict, keep_default_na=False)\n",
    "    label_dict_df = label_dict_df.set_index('label')\n",
    "    \n",
    "    label_dict = label_dict_df.to_dict(orient='index')\n",
    "    retval['label_dict'] = label_dict\n",
    "    \n",
    "    kana_dirs = os.listdir(etl_numpy_root)\n",
    "    \n",
    "    train_set = None\n",
    "    train_label = None\n",
    "    \n",
    "    val_set = None\n",
    "    val_label = None\n",
    "    \n",
    "    is_first = True\n",
    "    \n",
    "    for kana_dir in kana_dirs:\n",
    "        dir_label = int(label_dict[kana_dir]['index'])\n",
    "        \n",
    "        kana_np_path = os.path.join(etl_numpy_root, kana_dir)\n",
    "        \n",
    "        img_ds_path = os.path.join(kana_np_path, 'img_ds.npy')\n",
    "        img_arr = np.load(img_ds_path)\n",
    "        print('img_arr.shape', img_arr.shape)\n",
    "        \n",
    "        label_ds_path = os.path.join(kana_np_path, 'label_ds.npy')\n",
    "        label_arr = np.load(label_ds_path)\n",
    "        print('label_arr.shape', label_arr.shape)\n",
    "        \n",
    "        split_idx = int(len(img_arr) * val_ratio)\n",
    "        \n",
    "        _train_set, _val_set = np.split(img_arr, [split_idx], axis=0)\n",
    "        _train_label, _val_label = np.split(label_arr, [split_idx], axis=0)\n",
    "        \n",
    "        del img_arr\n",
    "        del label_arr\n",
    "        \n",
    "        if is_first:\n",
    "            train_set = _train_set\n",
    "            train_label = _train_label\n",
    "\n",
    "            val_set = _val_set\n",
    "            val_label = _val_label\n",
    "            \n",
    "            is_first = False\n",
    "        else:\n",
    "            train_set = np.concatenate((train_set, _train_set), axis=0)\n",
    "            train_label = np.concatenate((train_label, _train_label), axis=0)\n",
    "\n",
    "            val_set = np.concatenate((val_set, _val_set), axis=0)\n",
    "            val_label = np.concatenate((val_label, _val_label), axis=0)\n",
    "        \n",
    "    retval['train_set'] = train_set\n",
    "    retval['train_label'] = train_label\n",
    "    \n",
    "    retval['val_set'] = val_set\n",
    "    retval['val_label'] = val_label\n",
    "    \n",
    "    return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict = load_numpy_etl()\n",
    "print(ds_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_dict['train_set'].shape)\n",
    "print(ds_dict['val_set'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def kana_keras_model():\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Conv2D(\n",
    "            filters=32, \n",
    "            kernel_size=5, \n",
    "            padding='same',\n",
    "            activation=tf.nn.relu,\n",
    "            input_shape=(64,64,1,), \n",
    "            data_format='channels_last'\n",
    "            ),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.MaxPool2D(\n",
    "            pool_size=2,\n",
    "            ),\n",
    "        keras.layers.Conv2D(\n",
    "            filters=16, \n",
    "            kernel_size=5, \n",
    "            padding='same',\n",
    "            activation=tf.nn.relu,\n",
    "            ),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.MaxPool2D(\n",
    "            pool_size=(2, 2),\n",
    "            ),\n",
    "        keras.layers.Conv2D(\n",
    "            filters=16, \n",
    "            kernel_size=3, \n",
    "            padding='same',\n",
    "            activation=tf.nn.relu,\n",
    "            ),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.MaxPool2D(\n",
    "            pool_size=2,\n",
    "            ),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(\n",
    "            units=128, \n",
    "            activation=tf.nn.relu,\n",
    "            ),\n",
    "        keras.layers.Dropout(0.15),\n",
    "        keras.layers.Dense(\n",
    "            units=46,\n",
    "            activation=tf.nn.softmax\n",
    "        )\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = kana_keras_model()\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(ds_dict['train_set'], ds_dict['train_label'], \n",
    "          epochs=5, \n",
    "          batch_size=512, \n",
    "          validation_data=(ds_dict['val_set'], ds_dict['val_label']),\n",
    "          shuffle=True,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('kana_classifier_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
